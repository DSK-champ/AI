{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr2mJKUyLafJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Itteration 0\n",
        "w = -1\n",
        "\n",
        "def dLdw(w):\n",
        "  return 4*(math.pow((w - 2),3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------#\n",
        "#        n = 0.01       #\n",
        "#-----------------------#\n",
        "\n",
        "n = 0.01;\n",
        "w = -1;\n",
        "print(\"w0 = \",w)\n",
        "\n",
        "for i in range(5):\n",
        "  grad = dLdw(w)\n",
        "  w = w - (n * grad)\n",
        "  w = round(w,4)\n",
        "  print(f\"gradient = {grad:>25}       w{i+1} = \",w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTi8vAJjMLD9",
        "outputId": "10cefb94-9257-41c6-ddb3-e451d0595957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 =  -1\n",
            "gradient =                    -108.0       w1 =  0.08\n",
            "gradient =       -28.311551999999995       w2 =  0.3631\n",
            "gradient =          -17.543911885636       w3 =  0.5385\n",
            "gradient =            -12.4869522335       w4 =  0.6634\n",
            "gradient =           -9.551341247584       w5 =  0.7589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------#\n",
        "#        n = 0.1        #\n",
        "#-----------------------#\n",
        "\n",
        "n = 0.1;\n",
        "w = -1;\n",
        "print(\"w0 = \",w)\n",
        "\n",
        "for i in range(5):\n",
        "  grad = dLdw(w)\n",
        "  w = w - (n * grad)\n",
        "  w = round(w,4)\n",
        "  print(f\"gradient = {grad:>25}       w{i+1} = \",w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XAtdXwONwc5",
        "outputId": "1db71191-3c3b-4f7a-aedc-566f3d41697d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 =  -1\n",
            "gradient =                    -108.0       w1 =  9.8\n",
            "gradient =        1898.2080000000005       w2 =  -180.0208\n",
            "gradient =       -24122540.695321757       w3 =  2412074.0487\n",
            "gradient =     5.613462419295584e+19       w4 =  -5.613462419293172e+18\n",
            "gradient =    -7.075423664964186e+56       w5 =  7.075423664964187e+55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------#\n",
        "#        n = 0.5        #\n",
        "#-----------------------#\n",
        "\n",
        "n = 0.5;\n",
        "w = -1;\n",
        "print(\"w0 = \",w)\n",
        "\n",
        "for i in range(5):\n",
        "  grad = dLdw(w)\n",
        "  w = w - (n * grad)\n",
        "  w = round(w,4)\n",
        "  print(f\"gradient = {grad:>25}       w{i+1} = \",w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6JiHCZN5jZ",
        "outputId": "1ef8bc96-1de2-48b8-e7e4-0643617e2497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w0 =  -1\n",
            "gradient =                    -108.0       w1 =  53.0\n",
            "gradient =                  530604.0       w2 =  -265249.0\n",
            "gradient =    -7.465021810643301e+16       w3 =  3.7325109052951256e+16\n",
            "gradient =    2.0799995795061626e+50       w4 =  -1.0399997897530813e+50\n",
            "gradient =   -4.499453271163744e+150       w5 =  2.249726635581872e+150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Learning Rate**        | **0.01**                                                                               | **0.1**                                                                                                         | **0.5**                                                                                                |\n",
        "| ------------------------ | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
        "| **Observation**     <br><br>.     | Weight updates change very gradually. <br><br>Values remain controlled and do not explode.      | Weight values increase rapidly. <br><br>Values grow very large (exploding updates).                                     | Weight values explode extremely fast. <br><br>Divergence occurs almost immediately.                            |\n",
        "| **Speed of Convergence** <br><br>. | ‚ùå Slow                                                                                  | ‚ö†Ô∏è Fast initially, but unstable                                                                                 | ‚ùå No convergence                                                                                       |\n",
        "| **Oscillations**   <br><br>.      | üü¢ Absent or very minimal                                                                | ‚ùå  Present , we desiere minimum oscillations                                                                                                     | ‚ùå Severe , means it is very unstable                                                                                              |\n",
        "| **Overshooting Minimum** <br><br>.| üü¢ No MInimum is eventually acheived | ‚ùå Yes                                                                                                          | ‚ùå Extreme                                                                                             |\n",
        "| **Conclusion**      <br><br>.     | A small learning rate leads to stable but slow convergence. <br><br>It is safe but inefficient. | The learning rate is too large, causing the updates to overshoot the minimum and diverge <br><br>instead of converging. | A very large learning rate results in high instability, massive overshooting,       <br><br> and complete divergence. |\n"
      ],
      "metadata": {
        "id": "AKvwOMcyPB4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The learning rate ***0.01*** provides the ***best*** trade-off between stability and speed for this loss function."
      ],
      "metadata": {
        "id": "UmOJXqQSUPL6"
      }
    }
  ]
}